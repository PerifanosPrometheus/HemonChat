{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "[HemOnc.org](https://hemonc.org/wiki/Main_Page) is the largest freely available medical wiki of interventions, regimens, and general information relevant to the fields of hematology and oncology. It is designed for easy use and intended for healthcare professionals.\n",
    "\n",
    "For data professional, the hemonc team has released their [ontology](https://hemonc.org/wiki/Ontology) which is freely available for academic and non-commercial use via [HemOnc Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/FPO4HB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this Notebook\n",
    "\n",
    "This Notebook processes the aforementioned Ontology released by Hemonc to produce a training dataset for finetuning an LLM to answer questions about Oncological Drugs. The methodology utilized is based on [GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph\n",
    "Alignment via Neighborhood Partitioning and Generative Subgraph Encoding](https://arxiv.org/pdf/2402.06764)\n",
    "\n",
    "### Requirements to run this notebook\n",
    "\n",
    "This Notebook requires the following files from [HemOnc Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/FPO4HB):\n",
    "1) concept_stage.tab\n",
    "2) concept_relationship.tab\n",
    "\n",
    "The notebook also expects those files to be saved as .csv and to be renamed in the following way:\n",
    "1) concept_stage.tab -> concept.csv\n",
    "2) concept_relationship_stage.tab -> concept_relationship.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import anthropic\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Files containing the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_relationship = pd.read_csv(\"../../data/hemonc/concept_relationship.csv\") \n",
    "concept = pd.read_csv(\"../../data/hemonc/concept.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Cleaning\n",
    "\n",
    "### Input Data\n",
    "\n",
    "1) concept.csv -> Table containing one row per Hemonc Concept. A Hemonc Concept is a discrete unitary piece of information, e.g a drug, a regimen, a diagnosis code etc.\n",
    "2) concept_relationship.csv -> Table containing one row per relationship between Hemonc Concepts or between an Hemonc Concept and an external vocabulary(NDC/other codes).\n",
    "\n",
    "### Processing & Cleaning description\n",
    "\n",
    "The concept relationships table contains two types of relationships:\n",
    "- relationships between Hemonc concepts.\n",
    "- relationships between Hemonc concepts and NDC/other external vocabulary codes. In these scenarios the NDC/other external vocabulary codes do not reference back to any Hemonc Concept.\n",
    "\n",
    "The set of relationships generate tree-like structures. Since the relationships can be reciprocal, we can have cycles within the tree.\n",
    "\n",
    "The goal of the data processing & cleaning is to peform recursion of the knowledge tree to produce \"graph embeddings\". A graph embedding is a piece of text encoding the knowledge graph.\n",
    "\n",
    "The graph embeddings will then be fed to an LLM to produce Q&A training data pairs.\n",
    "\n",
    "To generate graph embeddings we will do the following:\n",
    "1) Extract Neighborhood Subgraphs: For each node (concept) we will extract its k-hop neighborhood to capture the local structure and relationships.\n",
    "   - We use $k=2$ in this notebook\n",
    "2) Partition Large Subgraphs: If a subgraph exceeds a predefined node limit (N_max), we will partition it into smaller, manageable subgraphs to ensure they fit within the LLM's context window.\n",
    "   - We use $N_{max}=100$ in this notebook\n",
    "3) Generate Graph Embeddings: We will translate each subgraph into the textual representation described earlier.\n",
    "4) Generate Q&A Pairs: For each graph embedding, we will create question and answer pairs to later train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean-up Knowledge tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure consistent data types for merging\n",
    "concept['concept_code'] = concept['concept_code'].astype(str)\n",
    "concept_relationship['concept_code_1'] = concept_relationship['concept_code_1'].astype(str)\n",
    "concept_relationship['concept_code_2'] = concept_relationship['concept_code_2'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out invalid relationships\n",
    "valid_relationships = concept_relationship[concept_relationship['invalid_reason'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to add concept names and classes for concept_code_1\n",
    "relationships = valid_relationships.merge(\n",
    "    concept[['concept_code', 'concept_name', 'concept_class_id']],\n",
    "    left_on='concept_code_1',\n",
    "    right_on='concept_code',\n",
    "    how='left'\n",
    ").rename(columns={\n",
    "    'concept_name': 'concept_name_1',\n",
    "    'concept_class_id': 'concept_class_id_1'\n",
    "}).drop(columns=['concept_code'])\n",
    "\n",
    "# Merge to add concept names and classes for concept_code_2\n",
    "relationships = relationships.merge(\n",
    "    concept[['concept_code', 'concept_name', 'concept_class_id']],\n",
    "    left_on='concept_code_2',\n",
    "    right_on='concept_code',\n",
    "    how='left'\n",
    ").rename(columns={\n",
    "    'concept_name': 'concept_name_2',\n",
    "    'concept_class_id': 'concept_class_id_2'\n",
    "}).drop(columns=['concept_code'])\n",
    "\n",
    "# Fill missing concept names with concept codes\n",
    "relationships['concept_name_2'] = relationships['concept_name_2'].combine_first(relationships['concept_code_2'])\n",
    "\n",
    "# Select relevant columns for analysis\n",
    "relationships = relationships[[\n",
    "    'concept_code_1', 'vocabulary_id_1', 'concept_name_1', 'concept_class_id_1',\n",
    "    'relationship_id', 'concept_code_2', 'vocabulary_id_2', 'concept_name_2', 'concept_class_id_2'\n",
    "]]\n",
    "\n",
    "# Ensure consistency in concept code data types\n",
    "relationships['concept_code_1'] = relationships['concept_code_1'].astype(str)\n",
    "\n",
    "# Exclude specific concept classes\n",
    "exclude_classes = [\n",
    "    'ReferenceDOI', 'PubMedCentralURL', 'Study Group', \n",
    "    'Duration', 'Author', 'Study', 'Reference'\n",
    "]\n",
    "relationships = relationships[~relationships['concept_class_id_1'].isin(exclude_classes)]\n",
    "\n",
    "# Some Hemonc concepts are pointing to themselves\n",
    "# This is likely due to human error of the curator of the ontology\n",
    "# We will be removing such rows\n",
    "index_of_rows_pointing_to_themselves = relationships[relationships['concept_code_1']==relationships['concept_code_2']].index\n",
    "relationships = relationships.drop(index_of_rows_pointing_to_themselves)\n",
    "\n",
    "# Drop duplicates\n",
    "relationships = relationships.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Neighborhood Subgraphs & Partition Large Subgraphs for Drug Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subgraphs(relationships, k=2, N_max=100, target_nodes=None):\n",
    "    \"\"\"\n",
    "    Generate subgraphs with a neighborhood of k hops and limit the size to N_max nodes.\n",
    "    \n",
    "    Parameters:\n",
    "    - relationships: pd.DataFrame, the relationships table.\n",
    "    - k: int, number of hops to consider.\n",
    "    - N_max: int, maximum number of nodes in the subgraph.\n",
    "    \n",
    "    Returns:\n",
    "    - subgraphs: dict, mapping each node to its subgraph.\n",
    "    \"\"\"\n",
    "    # Build adjacency list for the graph\n",
    "    adjacency_list = defaultdict(set)\n",
    "    for _, row in relationships.iterrows():\n",
    "        adjacency_list[row['concept_code_1']].add((row['concept_code_2'], row['relationship_id']))\n",
    "\n",
    "    # If no target_nodes provided, use all unique nodes in the table\n",
    "    if target_nodes is None:\n",
    "        target_nodes = relationships['concept_code_1'].unique()\n",
    "\n",
    "    # Generate subgraphs\n",
    "    subgraphs = {}\n",
    "    \n",
    "    for node in target_nodes:\n",
    "        if node not in adjacency_list:\n",
    "            continue  # Skip if the node is not in the adjacency list\n",
    "\n",
    "        # Perform BFS to gather neighbors up to k hops\n",
    "        visited = set()\n",
    "        queue = [(node, 0)]  # (current_node, current_depth)\n",
    "        subgraph_edges = []\n",
    "        \n",
    "        while queue:\n",
    "            current_node, depth = queue.pop(0)\n",
    "            if depth > k or current_node in visited:\n",
    "                continue\n",
    "            \n",
    "            visited.add(current_node)\n",
    "            for neighbor, relation in adjacency_list[current_node]:\n",
    "                subgraph_edges.append((current_node, relation, neighbor))\n",
    "                if neighbor not in visited:\n",
    "                    queue.append((neighbor, depth + 1))\n",
    "        \n",
    "        # Limit subgraph size to N_max nodes\n",
    "        unique_nodes = {edge[2] for edge in subgraph_edges}  # Gather unique nodes (concept_code_2)\n",
    "        if len(unique_nodes) > N_max:\n",
    "            subgraph_edges = subgraph_edges[:N_max]  # Truncate to fit N_max\n",
    "        \n",
    "        # Store subgraph as DataFrame for convenience\n",
    "        subgraphs[node] = pd.DataFrame(subgraph_edges, columns=['concept_code_1', 'relationship_id', 'concept_code_2'])\n",
    "    \n",
    "    return subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraphs = generate_subgraphs(relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_node = list(subgraphs.keys())[0]\n",
    "print(f\"Subgraph for node {example_node}:\\n\", subgraphs[example_node])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Graph Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_subgraph_to_text(subgraphs, concept):\n",
    "    \"\"\"\n",
    "    Encode subgraphs into the format shown in the GLAM paper.\n",
    "    \n",
    "    Parameters:\n",
    "    - subgraphs: dict, mapping each node to its subgraph DataFrame.\n",
    "    - concept: pd.DataFrame, the hemonc concept table.\n",
    "\n",
    "    Returns:\n",
    "    - glam_encoded_texts: dict, mapping each node to its GLAM-formatted text representation.\n",
    "    \"\"\"\n",
    "    glam_encoded_texts = {}\n",
    "\n",
    "    # Create a dictionary to map concept codes to their names and attributes\n",
    "    concept_details_df = concept[['concept_code', 'concept_name', 'concept_class_id']]\n",
    "    concept_details = concept_details_df.set_index('concept_code').to_dict('index')\n",
    "\n",
    "    for node, subgraph in subgraphs.items():\n",
    "        if node not in concept_details:\n",
    "            continue  # Skip nodes not in the concept details\n",
    "\n",
    "        # Store concept name of node\n",
    "        node_concept_name = concept_details[node]['concept_name']\n",
    "\n",
    "        # Group targets of the same relationship\n",
    "        relationship_groups = {}\n",
    "        for _, row in subgraph.iterrows():\n",
    "            relationship = row['relationship_id']\n",
    "            target_name = concept_details.get(row['concept_code_2'], {}).get('concept_name', row['concept_code_2'])\n",
    "\n",
    "            if relationship not in relationship_groups:\n",
    "                relationship_groups[relationship] = []\n",
    "            relationship_groups[relationship].append(target_name)\n",
    "        \n",
    "        # Generate per-relationship encoding\n",
    "        grouped_sentences = []\n",
    "        for relationship, targets in relationship_groups.items():\n",
    "            target_list = \",\".join(sorted(set(targets)))  # Combine and deduplicate targets\n",
    "            grouped_sentences.append(f\"{node_concept_name}, [{relationship}], {target_list}.\")\n",
    "        \n",
    "        # Combine sentences into GLAM summary\n",
    "        adjacency_list_summary = \" \".join(grouped_sentences)\n",
    "        glam_encoded_texts[node] = f\"{adjacency_list_summary}\"\n",
    "    \n",
    "    return glam_encoded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode subgraphs into text\n",
    "encoded_texts = encode_subgraph_to_text(subgraphs, concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print example GLAM-formatted text for a node\n",
    "example_node = list(encoded_texts.keys())[0]\n",
    "print(f\"GLAM Encoded Text for node {example_node}:\\n\", encoded_texts[example_node])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use LLM to convert encodings into more coherent representations using summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_pickle(data, file_path):\n",
    "    \"\"\"\n",
    "    Save data to a pickle file.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The data to be saved.\n",
    "    - file_path: The path to the pickle file (e.g., '*.pkl').\n",
    "    \"\"\"\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "    print(f\"Data successfully saved to {file_path}\")\n",
    "    \n",
    "def summarize_encodings(\n",
    "        encoded_texts, \n",
    "        anthropic_client, \n",
    "        system_prompt=\"You are a medical oncology journal editor.\",\n",
    "        summarization_prompt=\"Given a sentence you respond with a concise and accurate rewritten version. Ensure human-readable names, reduce redundancy, and include synonyms or expanded terms where appropriate.\",\n",
    "        save_progress=True\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Summarize GLAM-formatted adjacency list encodings using an LLM.\n",
    "\n",
    "    Parameters:\n",
    "    - encoded_texts: dict, mapping each node to its GLAM-formatted adjacency list representation.\n",
    "    - anthropic_client: object, anthropic client.\n",
    "    - system_prompt: str, the system prompt defining the agent role.\n",
    "    - summarization_prompt: str, the prompt to guide the agent for summarization.\n",
    "\n",
    "    Returns:\n",
    "    - summarized_encodings: dict, mapping each node to its summarized representation.\n",
    "    \"\"\"\n",
    "    summarized_encodings = {}\n",
    "\n",
    "    for node, encoded_text in encoded_texts.items():\n",
    "        print(f\"Processing node: {node}...\")\n",
    "        # Construct the full prompt for the LLM\n",
    "        input_text = f\"{summarization_prompt}\\n---\\nSentence: {encoded_text}\"\n",
    "\n",
    "        try:\n",
    "            response = anthropic_client.messages.create(\n",
    "                model=\"claude-3-haiku-20240307\", #cheapest model. For this task we can probably also use Llama models.\n",
    "                max_tokens=2048,\n",
    "                system=system_prompt,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": input_text}\n",
    "                ]\n",
    "            )\n",
    "            summarized_encodings[node] = response.content[0].text\n",
    "\n",
    "            if save_progress:\n",
    "                save_to_pickle(summarized_encodings,'../../data/intermediate/summarized_encodings.pkl')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing node {node}: {e}\")\n",
    "            summarized_encodings[node] = None  # Handle errors gracefully\n",
    "\n",
    "    return summarized_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Anthropic client\n",
    "anthropic_client = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize encoded texts\n",
    "summarized_texts = summarize_encodings(encoded_texts, anthropic_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the summarized encoding for an example node\n",
    "example_node = list(summarized_texts.keys())[0]\n",
    "print(f\"Summarized encoding for node {example_node}:\\n\", summarized_texts[example_node])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_pickle(summarized_texts,'../../data/intermediate/summarized_texts.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
